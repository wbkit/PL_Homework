<!DOCTYPE html>
<!-- saved from url=(0446)https://disqus.com/embed/comments/?base=default&f=seitasplace&t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&s_o=default#version=9068118211410bc5f67f5bb8d6806cba -->
<html lang="en" dir="ltr" class="js no-touch localstorage sessionstorage contenteditable use-opacity-transitions" style="--publisher-color:rgb(42,122,226); --publisher-color-safe:rgb(42,122,226);"><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Disqus Comments</title>

    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <style>
        .alert--warning {
            border-radius: 3px;
            padding: 10px 15px;
            margin-bottom: 10px;
            background-color: #FFE070;
            color: #A47703;
        }

        .alert--warning a,
        .alert--warning a:hover,
        .alert--warning strong {
            color: #A47703;
            font-weight: bold;
        }

        .alert--error p,
        .alert--warning p {
            margin-top: 5px;
            margin-bottom: 5px;
        }
        
        </style>
    
    <style>
        
        html {
            overflow: hidden;
        }
        

        #error {
            display: none;
        }

        .clearfix:after {
            content: "";
            display: block;
            height: 0;
            clear: both;
            visibility: hidden;
        }

        
    </style>

<script crossorigin="anonymous" id="bootstrap-script" data-app="lounge" src="./lounge.load.9068118211410bc5f67f5bb8d6806cba.js"></script><meta http-equiv="Content-Security-Policy" content="script-src https:;"><link rel="stylesheet" href="./lounge.7ab903feba7624935283ca4c7d8c7203.css"><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="lounge/main" src="./lounge.bundle.920cdf639b386b42eddc25a8b2755561.js"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="remote/config" src="./config.js"></script><style id="css_1639302167354"></style><!--<base target="_parent">--><base href="." target="_parent"></head>
<body class="sans-serif">
    

    
    <div id="error" class="alert--error">
        <p>We were unable to load Disqus. If you are a moderator please see our <a href="https://docs.disqus.com/help/83/"> troubleshooting guide</a>. </p>
    </div>

    
    <script type="text/json" id="disqus-forumData">{"session":{"canModerate":false,"audienceSyncVerified":false,"mustVerify":false,"canReply":true,"mustVerifyEmail":false},"forum":{"aetBannerConfirmation":null,"founder":"157207291","twitterName":"","commentsLinkOne":"1 Comment","guidelines":null,"favicon":{"permalink":"https://disqus.com/api/forums/favicons/seitasplace.jpg","cache":"//a.disquscdn.com/1638827995/images/favicon-default.png"},"commentsLinkZero":"0 Comments","disableDisqusBranding":false,"id":"seitasplace","createdAt":"2015-05-12T19:57:55.234659","category":null,"aetBannerEnabled":false,"aetBannerTitle":null,"raw_guidelines":null,"initialCommentCount":null,"votingType":null,"daysUnapproveNewUsers":null,"installCompleted":true,"moderatorBadgeText":"","commentPolicyText":null,"aetEnabled":false,"channel":null,"sort":1,"description":"","newPolicy":true,"raw_description":"","customFont":null,"language":"en","adsReviewStatus":1,"commentsPlaceholderTextEmpty":null,"daysAlive":0,"forumCategory":null,"colorScheme":"auto","pk":"3598555","commentsPlaceholderTextPopulated":null,"permissions":{},"commentPolicyLink":null,"aetBannerDescription":null,"name":"seitasplace","commentsLinkMultiple":"{num} Comments","settings":{"threadRatingsEnabled":false,"adsDRNativeEnabled":true,"adsPositionSidebarEnabled":true,"disable3rdPartyTrackers":false,"adsVideoEnabled":true,"adsProductVideoEnabled":true,"adsPositionTopEnabled":true,"ssoRequired":false,"unapproveLinks":false,"adsPositionRecommendationsEnabled":true,"linkAffiliationEnabled":false,"adsProductLinksThumbnailsEnabled":true,"hasCustomAvatar":false,"organicDiscoveryEnabled":true,"adsProductDisplayEnabled":false,"adsProductLinksEnabled":true,"threadReactionsEnabled":false,"adsEnabled":true,"disableSocialShare":false,"allowAnonPost":true,"adsProductStoriesEnabled":true,"adultContent":false,"allowAnonVotes":false,"gifPickerEnabled":true,"mustVerify":true,"badgesEnabled":false,"mustVerifyEmail":true,"unapproveNewUsersEnabled":false,"audienceSyncEnabled":false,"behindClickEnabled":false,"mediaembedEnabled":true,"adsPositionBottomEnabled":true,"discoveryLocked":false,"validateAllPosts":true,"adsSettingsLocked":false,"isVIP":false,"adsPositionInthreadEnabled":true},"organizationId":2619231,"typeface":"auto","url":"http://danieltakeshi.github.io/","daysThreadAlive":0,"avatar":{"small":{"permalink":"https://disqus.com/api/forums/avatars/seitasplace.jpg?size=32","cache":"//a.disquscdn.com/1638827995/images/noavatar32.png"},"large":{"permalink":"https://disqus.com/api/forums/avatars/seitasplace.jpg?size=92","cache":"//a.disquscdn.com/1638827995/images/noavatar92.png"}},"signedUrl":"http://disq.us/?url=http%3A%2F%2Fdanieltakeshi.github.io%2F&key=h4IVfrPj8rJ2D2f-I_Jvpw"}}</script>

    <script type="text/json" id="disqus-threadData">{"cursor":{"hasPrev":false,"prev":null,"total":19,"hasNext":false,"next":"1:0:0"},"code":0,"response":{"lastModified":1638347191,"posts":[{"editableUntil":"2018-02-14T15:48:28","dislikes":0,"thread":"5673992953","numReports":0,"likes":0,"message":"\u003cp>\u2211t=0T\u22121\ud835\udd3c\u03c4[(\u2207\u03b8log\u03c0\u03b8(at|st))2]\ud835\udd3c\u03c4[(Rt(\u03c4)\u2212b(st))2]\u003c/p>\u003cp>Shouldnt the optimal baseline be obtained by differentiating this quantity( the variance) and finding the minima which would not be similar to the expectation of the rewards or the average trajectory ?\u003c/p>","id":"3747086801","createdAt":"2018-02-07T15:48:28","author":{"username":"navneetmkumar","about":"","name":"Navneet M Kumar","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2018-02-07T15:46:36","profileUrl":"https://disqus.com/by/navneetmkumar/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"279442539","avatar":{"permalink":"https://disqus.com/api/users/avatars/navneetmkumar.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/navneetmkumar.jpg","cache":"//a.disquscdn.com/1638827995/images/noavatar128.png"},"cache":"//a.disquscdn.com/1638827995/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/navneetmkumar.jpg","cache":"//a.disquscdn.com/1638827995/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/navneetmkumar.jpg","cache":"//a.disquscdn.com/1638827995/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"\u2211t=0T\u22121\ud835\udd3c\u03c4[(\u2207\u03b8log\u03c0\u03b8(at|st))2]\ud835\udd3c\u03c4[(Rt(\u03c4)\u2212b(st))2]\n\n\n\n\nShouldnt the optimal baseline be obtained by differentiating this quantity( the variance) and finding the minima which would not be similar to the expectation of the rewards or the average trajectory ?","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"seitasplace","depth":0,"points":0,"moderationLabels":[],"isEdited":true,"sb":false},{"editableUntil":"2019-01-26T04:42:02","dislikes":0,"thread":"5673992953","numReports":0,"likes":0,"message":"\u003cp>I don't think it's as simple as that. There is a very hefty theory that Bartlett and others did many years ago that derived the optimal minimum variance. I have not had time to go through those papers because it's not directly relevant to my work.\u003c/p>","id":"4293134529","createdAt":"2019-01-19T04:42:02","author":{"username":"seitad","about":"PhD student at UC Berkeley","name":"Daniel Seita","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-05-11T19:45:37","profileUrl":"https://disqus.com/by/seitad/","url":"http://danieltakeshi.github.io","location":"Berkeley, California","isPrivate":false,"signedUrl":"http://disq.us/?url=http%3A%2F%2Fdanieltakeshi.github.io&key=gVNxbx2Uf4ou4bKeM8DRng","isPrimary":true,"isAnonymous":false,"id":"157207291","avatar":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar128.jpg?1452441681"},"cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar92.jpg?1452441681","large":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar92.jpg?1452441681"},"small":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar32.jpg?1452441681"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3747086801,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"I don't think it's as simple as that. There is a very hefty theory that Bartlett and others did many years ago that derived the optimal minimum variance. I have not had time to go through those papers because it's not directly relevant to my work.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"seitasplace","depth":1,"points":0,"moderationLabels":[],"isEdited":true,"sb":false},{"editableUntil":"2018-07-30T00:29:01","dislikes":0,"thread":"5673992953","numReports":0,"likes":0,"message":"\u003cp>Thanks a lot. It really help me to understand the concept better\u003c/p>","id":"4001599207","createdAt":"2018-07-23T00:29:01","author":{"username":"damienshen","about":"","name":"Damien Shen","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2016-07-16T09:54:23","profileUrl":"https://disqus.com/by/damienshen/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"215271782","avatar":{"permalink":"https://disqus.com/api/users/avatars/damienshen.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/damienshen.jpg","cache":"//a.disquscdn.com/1638827995/images/noavatar128.png"},"cache":"//a.disquscdn.com/1638827995/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/damienshen.jpg","cache":"//a.disquscdn.com/1638827995/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/damienshen.jpg","cache":"//a.disquscdn.com/1638827995/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Thanks a lot. It really help me to understand the concept better","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"seitasplace","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2019-01-26T04:38:17","dislikes":0,"thread":"5673992953","numReports":0,"likes":0,"message":"\u003cp>You're welcome!\u003c/p>","id":"4293132104","createdAt":"2019-01-19T04:38:17","author":{"username":"seitad","about":"PhD student at UC Berkeley","name":"Daniel Seita","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-05-11T19:45:37","profileUrl":"https://disqus.com/by/seitad/","url":"http://danieltakeshi.github.io","location":"Berkeley, California","isPrivate":false,"signedUrl":"http://disq.us/?url=http%3A%2F%2Fdanieltakeshi.github.io&key=gVNxbx2Uf4ou4bKeM8DRng","isPrimary":true,"isAnonymous":false,"id":"157207291","avatar":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar128.jpg?1452441681"},"cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar92.jpg?1452441681","large":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar92.jpg?1452441681"},"small":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar32.jpg?1452441681"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4001599207,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"You're welcome!","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"seitasplace","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-10-11T17:53:53","dislikes":0,"thread":"5673992953","numReports":0,"likes":0,"message":"\u003cp>The three ways are not equivalent algebraically, and I think this is an important distinction. They are not equivalent, because ii and iii makes assumptions of the graphical model.\u003c/p>","id":"4129062643","createdAt":"2018-10-04T17:53:53","author":{"username":"ge_yang","about":"","name":"Ge Yang","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2014-04-21T03:04:41","profileUrl":"https://disqus.com/by/ge_yang/","url":"http://www.nantijiazi.com","location":"Chicago","isPrivate":false,"signedUrl":"http://disq.us/?url=http%3A%2F%2Fwww.nantijiazi.com&key=EiGbe3zQFUAhuuFNOgkVpQ","isPrimary":true,"isAnonymous":false,"id":"104007738","avatar":{"permalink":"https://disqus.com/api/users/avatars/ge_yang.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/ge_yang.jpg","cache":"https://c.disquscdn.com/uploads/users/10400/7738/avatar128.jpg?1538675635"},"cache":"https://c.disquscdn.com/uploads/users/10400/7738/avatar92.jpg?1538675635","large":{"permalink":"https://disqus.com/api/users/avatars/ge_yang.jpg","cache":"https://c.disquscdn.com/uploads/users/10400/7738/avatar92.jpg?1538675635"},"small":{"permalink":"https://disqus.com/api/users/avatars/ge_yang.jpg","cache":"https://c.disquscdn.com/uploads/users/10400/7738/avatar32.jpg?1538675635"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"The three ways are not equivalent algebraically, and I think this is an important distinction. They are not equivalent, because ii and iii makes assumptions of the graphical model.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"seitasplace","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2019-01-26T04:39:26","dislikes":0,"thread":"5673992953","numReports":0,"likes":0,"message":"\u003cp>Sorry for my late response. What are you referring to in the post when you mention \"three ways\"?\u003c/p>","id":"4293132836","createdAt":"2019-01-19T04:39:26","author":{"username":"seitad","about":"PhD student at UC Berkeley","name":"Daniel Seita","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-05-11T19:45:37","profileUrl":"https://disqus.com/by/seitad/","url":"http://danieltakeshi.github.io","location":"Berkeley, California","isPrivate":false,"signedUrl":"http://disq.us/?url=http%3A%2F%2Fdanieltakeshi.github.io&key=gVNxbx2Uf4ou4bKeM8DRng","isPrimary":true,"isAnonymous":false,"id":"157207291","avatar":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar128.jpg?1452441681"},"cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar92.jpg?1452441681","large":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar92.jpg?1452441681"},"small":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar32.jpg?1452441681"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4129062643,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Sorry for my late response. What are you referring to in the post when you mention \"three ways\"?","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"seitasplace","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-10-19T21:08:37","dislikes":0,"thread":"5673992953","numReports":0,"likes":0,"message":"\u003cp>Thanks so much for putting this together! It was very helpful when trying to learn about these topics.  One thing I had a bit of confusion about: when you introduce the baseline into the calculation of expectation of cumulative reward of the trajectory, the left-hand side is technically no longer the expectation of R(tau), correct? It is now the expectation of a new estimate, R'(tau), which has less variance.\u003c/p>","id":"4142228096","createdAt":"2018-10-12T21:08:37","author":{"username":"disqus_Jk9goUcHDo","about":"","name":"krishnan","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-03-15T05:46:04","profileUrl":"https://disqus.com/by/disqus_Jk9goUcHDo/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"149082056","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_Jk9goUcHDo.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_Jk9goUcHDo.jpg","cache":"https://c.disquscdn.com/uploads/users/14908/2056/avatar128.jpg?1539378518"},"cache":"https://c.disquscdn.com/uploads/users/14908/2056/avatar92.jpg?1539378518","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_Jk9goUcHDo.jpg","cache":"https://c.disquscdn.com/uploads/users/14908/2056/avatar92.jpg?1539378518"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_Jk9goUcHDo.jpg","cache":"https://c.disquscdn.com/uploads/users/14908/2056/avatar32.jpg?1539378518"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Thanks so much for putting this together! It was very helpful when trying to learn about these topics.  One thing I had a bit of confusion about: when you introduce the baseline into the calculation of expectation of cumulative reward of the trajectory, the left-hand side is technically no longer the expectation of R(tau), correct? It is now the expectation of a new estimate, R'(tau), which has less variance.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"seitasplace","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2019-01-26T04:40:09","dislikes":0,"thread":"5673992953","numReports":0,"likes":0,"message":"\u003cp>The baseline itself should maintain equality if the original expression was an equality. The *discount* factors make it an inequality/approximation. Which specific area of the post are you referring to?\u003c/p>","id":"4293133321","createdAt":"2019-01-19T04:40:09","author":{"username":"seitad","about":"PhD student at UC Berkeley","name":"Daniel Seita","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-05-11T19:45:37","profileUrl":"https://disqus.com/by/seitad/","url":"http://danieltakeshi.github.io","location":"Berkeley, California","isPrivate":false,"signedUrl":"http://disq.us/?url=http%3A%2F%2Fdanieltakeshi.github.io&key=gVNxbx2Uf4ou4bKeM8DRng","isPrimary":true,"isAnonymous":false,"id":"157207291","avatar":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar128.jpg?1452441681"},"cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar92.jpg?1452441681","large":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar92.jpg?1452441681"},"small":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar32.jpg?1452441681"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4142228096,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"The baseline itself should maintain equality if the original expression was an equality. The *discount* factors make it an inequality/approximation. Which specific area of the post are you referring to?","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"seitasplace","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2019-01-10T13:12:50","dislikes":0,"thread":"5673992953","numReports":0,"likes":0,"message":"\u003cp>Really good effort.\u003cbr>I have a small doubt,how is the \"Reward to go\" actually calculated in Deep Reinforcement Learning.Let it be the Monte Carlo reward or Q values or the advantage.How can we estimate the reward for future state from the current state without knowing the future states.\u003c/p>","id":"4267479510","createdAt":"2019-01-03T13:12:50","author":{"name":"Sudharshan","url":"","profileUrl":"","emailHash":"","avatar":{"small":{"permalink":"//a.disquscdn.com/1638827995/images/noavatar32.png","cache":"//a.disquscdn.com/1638827995/images/noavatar32.png"},"large":{"permalink":"//a.disquscdn.com/1638827995/images/noavatar92.png","cache":"//a.disquscdn.com/1638827995/images/noavatar92.png"},"permalink":"//a.disquscdn.com/1638827995/images/noavatar92.png","cache":"//a.disquscdn.com/1638827995/images/noavatar92.png"},"signedUrl":"","isAnonymous":true},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Really good effort.\nI have a small doubt,how is the \"Reward to go\" actually calculated in Deep Reinforcement Learning.Let it be the Monte Carlo reward or Q values or the advantage.How can we estimate the reward for future state from the current state without knowing the future states.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"seitasplace","depth":0,"points":0,"moderationLabels":["anonymous"],"isEdited":false,"sb":false},{"editableUntil":"2019-01-26T04:40:58","dislikes":0,"thread":"5673992953","numReports":0,"likes":0,"message":"\u003cp>You play the future states and compute the rewards after the trajectory is done. That's how it's often implemented in practice. Is that clear?\u003c/p>","id":"4293133852","createdAt":"2019-01-19T04:40:58","author":{"username":"seitad","about":"PhD student at UC Berkeley","name":"Daniel Seita","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-05-11T19:45:37","profileUrl":"https://disqus.com/by/seitad/","url":"http://danieltakeshi.github.io","location":"Berkeley, California","isPrivate":false,"signedUrl":"http://disq.us/?url=http%3A%2F%2Fdanieltakeshi.github.io&key=gVNxbx2Uf4ou4bKeM8DRng","isPrimary":true,"isAnonymous":false,"id":"157207291","avatar":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar128.jpg?1452441681"},"cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar92.jpg?1452441681","large":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar92.jpg?1452441681"},"small":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar32.jpg?1452441681"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4267479510,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"You play the future states and compute the rewards after the trajectory is done. That's how it's often implemented in practice. Is that clear?","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"seitasplace","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2019-01-26T13:36:56","dislikes":0,"thread":"5673992953","numReports":0,"likes":0,"message":"\u003cp>In approximation (i) of variance reduction proof part, E[X] seems to be 0 so it is ignored. Is this because the exception of log function's derivative is 0, so no matter it multiply b(s_t) or [R(\\tau)-b(s_t)] is still 0?\u003c/p>","id":"4293481946","createdAt":"2019-01-19T13:36:56","author":{"username":"disqus_dp7d6xECXP","about":"","name":"mike","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2019-01-19T13:36:09","profileUrl":"https://disqus.com/by/disqus_dp7d6xECXP/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"327750135","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_dp7d6xECXP.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_dp7d6xECXP.jpg","cache":"//a.disquscdn.com/1638827995/images/noavatar128.png"},"cache":"//a.disquscdn.com/1638827995/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_dp7d6xECXP.jpg","cache":"//a.disquscdn.com/1638827995/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_dp7d6xECXP.jpg","cache":"//a.disquscdn.com/1638827995/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"In approximation (i) of variance reduction proof part, E[X] seems to be 0 so it is ignored. Is this because the exception of log function's derivative is 0, so no matter it multiply b(s_t) or [R(\\tau)-b(s_t)] is still 0?","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"seitasplace","depth":0,"points":0,"moderationLabels":[],"isEdited":true,"sb":false},{"editableUntil":"2020-05-28T23:05:50","dislikes":0,"thread":"5673992953","numReports":0,"likes":0,"message":"\u003cp>This seems to be a mistake, since, as far as I understand, E[X] is precisely the policy gradient.\u003c/p>\u003cp>In addition, there is an (insignificant) typo in the section \"How to Introduce a Baseline\" under \"Step (ii)\". Twice, a formula says \\sum_{t'}, whereas it should say \\sum_{t'=0}.\u003c/p>","id":"4922842103","createdAt":"2020-05-21T23:05:50","author":{"username":"pastafarianist","about":"","name":"Pastafarianist","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2014-07-01T03:32:29","profileUrl":"https://disqus.com/by/pastafarianist/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"112705586","avatar":{"permalink":"https://disqus.com/api/users/avatars/pastafarianist.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/pastafarianist.jpg","cache":"https://c.disquscdn.com/uploads/users/11270/5586/avatar128.jpg?1590102352"},"cache":"https://c.disquscdn.com/uploads/users/11270/5586/avatar92.jpg?1590102352","large":{"permalink":"https://disqus.com/api/users/avatars/pastafarianist.jpg","cache":"https://c.disquscdn.com/uploads/users/11270/5586/avatar92.jpg?1590102352"},"small":{"permalink":"https://disqus.com/api/users/avatars/pastafarianist.jpg","cache":"https://c.disquscdn.com/uploads/users/11270/5586/avatar32.jpg?1590102352"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4293481946,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"This seems to be a mistake, since, as far as I understand, E[X] is precisely the policy gradient.\n\nIn addition, there is an (insignificant) typo in the section \"How to Introduce a Baseline\" under \"Step (ii)\". Twice, a formula says \\sum_{t'}, whereas it should say \\sum_{t'=0}.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"seitasplace","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2019-01-28T10:40:20","dislikes":0,"thread":"5673992953","numReports":0,"likes":3,"message":"\u003cp>The exchanging of the integral and derivative in the log-derivative trick is permissible in this way because the integral - being over a probability space - is between negative infinity and positive infinity i.e. the integral bounds are constants and not functions of \\theta. This means that when differentiating under the integral (Leibnitz trick) the two terms involving the partial derivatives of the integral bounds disappear and you are left with the term where the integral and differential have simply swapped places.\u003c/p>","id":"4299681379","createdAt":"2019-01-21T10:40:20","author":{"name":"Sebastian Lee","url":"","profileUrl":"","emailHash":"","avatar":{"small":{"permalink":"//a.disquscdn.com/1638827995/images/noavatar32.png","cache":"//a.disquscdn.com/1638827995/images/noavatar32.png"},"large":{"permalink":"//a.disquscdn.com/1638827995/images/noavatar92.png","cache":"//a.disquscdn.com/1638827995/images/noavatar92.png"},"permalink":"//a.disquscdn.com/1638827995/images/noavatar92.png","cache":"//a.disquscdn.com/1638827995/images/noavatar92.png"},"signedUrl":"","isAnonymous":true},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"The exchanging of the integral and derivative in the log-derivative trick is permissible in this way because the integral - being over a probability space - is between negative infinity and positive infinity i.e. the integral bounds are constants and not functions of \\theta. This means that when differentiating under the integral (Leibnitz trick) the two terms involving the partial derivatives of the integral bounds disappear and you are left with the term where the integral and differential have simply swapped places.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"seitasplace","depth":0,"points":3,"moderationLabels":["anonymous"],"isEdited":false,"sb":false},{"editableUntil":"2019-12-21T14:08:36","dislikes":0,"thread":"5673992953","numReports":0,"likes":1,"message":"\u003cp>Reference: \u003ca href=\"https://disq.us/url?url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FLeibniz_integral_rule%3A6FkLwDswhM2fG48GVqoUqPJ6v-A&amp;cuid=3598555\" rel=\"nofollow noopener\" title=\"https://en.wikipedia.org/wiki/Leibniz_integral_rule\">https://en.wikipedia.org/wi...\u003c/a>\u003c/p>","id":"4724105575","createdAt":"2019-12-14T14:08:36","author":{"username":"disqus_k5RkXja2sG","about":"","name":"Anonymous","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2019-12-12T13:07:00","profileUrl":"https://disqus.com/by/disqus_k5RkXja2sG/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"341978210","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_k5RkXja2sG.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_k5RkXja2sG.jpg","cache":"//a.disquscdn.com/1638827995/images/noavatar128.png"},"cache":"//a.disquscdn.com/1638827995/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_k5RkXja2sG.jpg","cache":"//a.disquscdn.com/1638827995/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_k5RkXja2sG.jpg","cache":"//a.disquscdn.com/1638827995/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4299681379,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Reference: https://en.wikipedia.org/wiki/Leibniz_integral_rule","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"seitasplace","depth":1,"points":1,"moderationLabels":["links"],"isEdited":false,"sb":false},{"editableUntil":"2020-01-19T23:47:55","dislikes":0,"thread":"5673992953","numReports":0,"likes":0,"message":"\u003cp>Thanks!\u003c/p>","id":"4754906193","createdAt":"2020-01-12T23:47:55","author":{"username":"seitad","about":"PhD student at UC Berkeley","name":"Daniel Seita","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-05-11T19:45:37","profileUrl":"https://disqus.com/by/seitad/","url":"http://danieltakeshi.github.io","location":"Berkeley, California","isPrivate":false,"signedUrl":"http://disq.us/?url=http%3A%2F%2Fdanieltakeshi.github.io&key=gVNxbx2Uf4ou4bKeM8DRng","isPrimary":true,"isAnonymous":false,"id":"157207291","avatar":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar128.jpg?1452441681"},"cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar92.jpg?1452441681","large":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar92.jpg?1452441681"},"small":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar32.jpg?1452441681"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4724105575,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Thanks!","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"seitasplace","depth":2,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2019-03-04T03:12:56","dislikes":0,"thread":"5673992953","numReports":0,"likes":0,"message":"\u003cp>Hi,\u003cbr>Thanks for your great blog. I was wondering one thing: what are the benefits of using advantage function instead of R_t() and b(s_t)? Are they the same? This somehow makes me confusing. Thanks!\u003c/p>","id":"4352851225","createdAt":"2019-02-25T03:12:56","author":{"name":"Cuong Hoang","url":"","profileUrl":"","emailHash":"","avatar":{"small":{"permalink":"//a.disquscdn.com/1638827995/images/noavatar32.png","cache":"//a.disquscdn.com/1638827995/images/noavatar32.png"},"large":{"permalink":"//a.disquscdn.com/1638827995/images/noavatar92.png","cache":"//a.disquscdn.com/1638827995/images/noavatar92.png"},"permalink":"//a.disquscdn.com/1638827995/images/noavatar92.png","cache":"//a.disquscdn.com/1638827995/images/noavatar92.png"},"signedUrl":"","isAnonymous":true},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Hi,\nThanks for your great blog. I was wondering one thing: what are the benefits of using advantage function instead of R_t() and b(s_t)? Are they the same? This somehow makes me confusing. Thanks!","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"seitasplace","depth":0,"points":0,"moderationLabels":["anonymous"],"isEdited":false,"sb":false},{"editableUntil":"2020-05-05T09:04:20","dislikes":0,"thread":"5673992953","numReports":0,"likes":0,"message":"\u003cp>Thank you so much. I was stuck trying to prove the baseline is unbiased when it's a function of the state. I spent a whole day on it. And only this blog finally unblocked me. Cheers !!\u003c/p>","id":"4892871874","createdAt":"2020-04-28T09:04:20","author":{"username":"vigneshskannan","about":"","name":"Vignesh S Kannan","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-01-14T11:13:16","profileUrl":"https://disqus.com/by/vigneshskannan/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"139371709","avatar":{"permalink":"https://disqus.com/api/users/avatars/vigneshskannan.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/vigneshskannan.jpg","cache":"https://c.disquscdn.com/uploads/users/13937/1709/avatar128.jpg?1588064661"},"cache":"https://c.disquscdn.com/uploads/users/13937/1709/avatar92.jpg?1588064661","large":{"permalink":"https://disqus.com/api/users/avatars/vigneshskannan.jpg","cache":"https://c.disquscdn.com/uploads/users/13937/1709/avatar92.jpg?1588064661"},"small":{"permalink":"https://disqus.com/api/users/avatars/vigneshskannan.jpg","cache":"https://c.disquscdn.com/uploads/users/13937/1709/avatar32.jpg?1588064661"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Thank you so much. I was stuck trying to prove the baseline is unbiased when it's a function of the state. I spent a whole day on it. And only this blog finally unblocked me. Cheers !!","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"seitasplace","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2020-05-29T16:23:01","dislikes":0,"thread":"5673992953","numReports":0,"likes":0,"message":"\u003cp>Thanks!\u003c/p>","id":"4923679941","createdAt":"2020-05-22T16:23:01","author":{"username":"seitad","about":"PhD student at UC Berkeley","name":"Daniel Seita","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-05-11T19:45:37","profileUrl":"https://disqus.com/by/seitad/","url":"http://danieltakeshi.github.io","location":"Berkeley, California","isPrivate":false,"signedUrl":"http://disq.us/?url=http%3A%2F%2Fdanieltakeshi.github.io&key=gVNxbx2Uf4ou4bKeM8DRng","isPrimary":true,"isAnonymous":false,"id":"157207291","avatar":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar128.jpg?1452441681"},"cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar92.jpg?1452441681","large":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar92.jpg?1452441681"},"small":{"permalink":"https://disqus.com/api/users/avatars/seitad.jpg","cache":"https://c.disquscdn.com/uploads/users/15720/7291/avatar32.jpg?1452441681"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4892871874,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Thanks!","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"seitasplace","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2020-07-23T09:39:57","dislikes":0,"thread":"5673992953","numReports":0,"likes":0,"message":"\u003cp>Thanks a lot Daniel ! One of the clearest explanation I encountered on the subject. I have a small remark to make though. In the part \"How to introduce a Baseline\" , you introduce a way to put the derivative inside the sum (equations (i), (ii), and (iii)). But after equation (i), can't you just simply use the linearity property of the derivative to switch the sum and the derivative ? It changes the sum over the rewards in equation (iii), from t' to T-1, to 0 to T-1. It does not influence the rest of your article though, so it's no big deal, but I just thought this part might be a bit over-complicated compared to simply using linearity property. Maybe I'm missing something ?\u003c/p>","id":"4993879335","createdAt":"2020-07-16T09:39:57","author":{"username":"nathanraynalcastang","about":"","name":"Nathan Raynal-Castang","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2020-07-16T09:39:25","profileUrl":"https://disqus.com/by/nathanraynalcastang/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"353124300","avatar":{"permalink":"https://disqus.com/api/users/avatars/nathanraynalcastang.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/nathanraynalcastang.jpg","cache":"//a.disquscdn.com/1638827995/images/noavatar128.png"},"cache":"//a.disquscdn.com/1638827995/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/nathanraynalcastang.jpg","cache":"//a.disquscdn.com/1638827995/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/nathanraynalcastang.jpg","cache":"//a.disquscdn.com/1638827995/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Thanks a lot Daniel ! One of the clearest explanation I encountered on the subject. I have a small remark to make though. In the part \"How to introduce a Baseline\" , you introduce a way to put the derivative inside the sum (equations (i), (ii), and (iii)). But after equation (i), can't you just simply use the linearity property of the derivative to switch the sum and the derivative ? It changes the sum over the rewards in equation (iii), from t' to T-1, to 0 to T-1. It does not influence the rest of your article though, so it's no big deal, but I just thought this part might be a bit over-complicated compared to simply using linearity property. Maybe I'm missing something ?","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"seitasplace","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false}],"thread":{"feed":"https://seitasplace.disqus.com/going_deeper_into_reinforcement_learning_fundamentals_of_policy_gradients_50/latest.rss","clean_title":"Going Deeper Into Reinforcement Learning: Fundamentals of Policy Gradients","dislikes":0,"likes":25,"message":"","ratingsEnabled":false,"isSpam":false,"isDeleted":false,"category":"3715284","adsDisabled":false,"author":"157207291","id":"5673992953","signedLink":"https://disq.us/?url=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&key=a6uSLsm9tnnujymKdikf7Q","createdAt":"2017-03-28T15:25:28","hasStreaming":false,"raw_message":"","isClosed":false,"link":"https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/","slug":"going_deeper_into_reinforcement_learning_fundamentals_of_policy_gradients_50","forum":"seitasplace","identifiers":[],"posts":19,"moderators":[157207291],"validateAllPosts":false,"title":"Going Deeper Into Reinforcement Learning: Fundamentals of Policy Gradients","highlightedPost":null}},"order":"asc"}</script>


    <div id="fixed-content"></div>


    
        
            
<script type="text/json" id="disqus-urls">{
    "root":"//disqus.com",
    "next":"https://c.disquscdn.com/next/current"
}</script>

        
        
        <script>!function () {
            var d = document;
            var toH = d.head.appendChild.bind(d.head);

            var v = window.location.href.match(/[#&?]version=([0-9a-f]{32})/);
            var src = 'https://c.disquscdn.com/next/embed/lounge.load';
            if (v)
                src += '.' + v[1];
            src += '.js';

            var s = d.createElement('script');
            s.crossOrigin = 'anonymous';
            s.id = 'bootstrap-script';
            s.setAttribute('data-app', 'lounge');
            s.src = src;
            toH(s);

            var m = d.createElement('meta');
            m.setAttribute('http-equiv', 'Content-Security-Policy');
            m.setAttribute('content', "script-src https:;");
            toH(m);
        }();</script>
    


<script src="./common.bundle.2f2f40d40785c9541a90e9086c8770a3.js"></script><div id="layout" data-tracking-area="layout"><div id="thread__container"><div><div id="thread__wrapper"><div id="placement-top" data-tracking-area="discovery-north"></div><div id="onboard" data-tracking-area="onboard"></div><div id="reactions__container"></div><div id="ratings__container"></div><div id="badges-message__container"></div><div id="highlighted-post" data-tracking-area="highlighted" class="highlighted-post" style="display: none;"></div><div id="global-alert"></div><div id="tos__container"></div><header id="main-nav" data-tracking-area="main-nav"><nav class="nav nav-primary"><ul><li class="nav-tab nav-tab--primary tab-conversation active" data-role="post-count"><a class="publisher-nav-color"><span class="comment-count">19 comments</span><span class="comment-count-placeholder">Comments</span></a></li><li class="nav-tab nav-tab--primary tab-general"><a href="https://disqus.com/home/forums/seitasplace/" class="publisher-nav-color" data-action="community-sidebar" data-forum="seitasplace" id="community-tab" name="seitasplace"><span class="community-name"><strong>seitasplace</strong></span><strong class="community-name-placeholder">Community</strong></a></li><li class="nav-tab nav-tab--primary tab-general"><a href="https://help.disqus.com/customer/portal/articles/466259-privacy-policy" rel="nofollow noopener noreferrer" target="_blank" class="publisher-nav-color privacy-policy" title="Disqus&#39; Privacy Policy"><i aria-hidden="true" class="icon icon-lock"></i><span class="clip privacy-policy-full hidden-md">&nbsp;Disqus' Privacy Policy</span><span class="clip privacy-policy-short">&nbsp;Privacy Policy</span></a></li><li class="nav-tab nav-tab--primary tab-user"><ul><li class="nav-tab nav-tab--primary notification-menu unread" data-role="notification-menu" style="display: list-item;"><a href="https://disqus.com/home/inbox/" class="notification-container" data-action="home" data-home-path="home/inbox/"><span class="notification-icon icon-comment" aria-hidden="true"></span><span class="notification-count" data-role="notification-count">1</span></a></li><li class="nav-tab nav-tab--primary dropdown user-menu media-collapsed" data-role="logout"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="dropdown-toggle" data-toggle="dropdown" role="menuitem" name="Login"><span class="dropdown-toggle-wrapper"><span>Login</span> </span> <span class="caret"></span></a><ul class="dropdown-menu"><li><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="auth:disqus">Disqus</a></li><li><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="auth:facebook">Facebook</a></li><li><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="auth:twitter">Twitter</a></li><li><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="auth:google">Google</a></li></ul></li></ul></li></ul></nav></header><section id="conversation" data-role="main" data-tracking-area="main"><div class="nav nav-secondary" data-tracking-area="secondary-nav"><ul><li id="favorite-button" class="nav-tab nav-tab--secondary favorite dropdown"><div class="thread-likes"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="favorite" title="Favorite this discussion" class="dropdown-toggle "><span class="label label-default"><span class="favorite-icon icon-heart-empty"></span> Favorite</span><span class="label label-favorited"><span class="favorite-icon icon-heart"></span> Favorite</span> <span class="label label-count">25</span></a><ul class="dropdown-menu dropdown-menu--coachmark"><li><div><h2 class="coachmark__heading">Discussion Favorited!</h2><p class="coachmark__description">Favoriting means this is a discussion worth sharing. It gets shared to your followers' Disqus feeds, and gives the creator kudos!</p></div> <a href="https://disqus.com/home/?utm_source=disqus_embed&amp;utm_content=recommend_btn" class="btn btn-primary coachmark__button" target="_blank" rel="noopener noreferrer">Find More Discussions</a></li></ul></div></div></li><li id="thread-share-bar" class="nav-tab nav-tab--secondary share-bar hidden-sm"><div class="thread-share-bar-buttons"><span class="thread-share__button share-twitter" data-action="share:twitter" tabindex="0"><span class="icon-twitter"></span><span class="share-text">Tweet</span></span><span class="thread-share__button share-facebook" data-action="share:facebook" tabindex="0"><span class="icon-facebook"></span><span class="share-text">Share</span></span></div></li><li data-role="post-sort" class="nav-tab nav-tab--secondary dropdown sorting pull-right"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="dropdown-toggle" data-toggle="dropdown">Sort by Oldest<span class="caret"></span></a><ul class="dropdown-menu pull-right"><li class=""><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="sort" data-sort="popular">Best<i aria-hidden="true" class="icon-checkmark"></i></a></li><li class=""><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="sort" data-sort="desc">Newest<i aria-hidden="true" class="icon-checkmark"></i></a></li><li class="selected"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="sort" data-sort="asc">Oldest<i aria-hidden="true" class="icon-checkmark"></i></a></li></ul></li></ul></div><div id="posts"><div id="form" class="textarea-outer-wrapper--top-level"><form class="reply"><div class="postbox"><div role="alert"></div><div class="ratings-wrapper" data-role="ratings-container"></div><div class="compose-wrapper"><div class="avatar"><span class="user"><img data-role="user-avatar" src="./noavatar92.png" alt="Avatar"></span></div><div class="textarea-outer-wrapper"><div class="textarea-wrapper" data-role="textarea" dir="auto"><div><span class="placeholder">Join the discussion</span><div class="textarea" tabindex="0" role="textbox" aria-multiline="true" contenteditable="PLAINTEXT-ONLY" data-role="editable" aria-label="Join the discussion" style="overflow: auto; overflow-wrap: break-word; max-height: 350px;"><p><br></p></div><div style="display: none;"><ul class="user-mention__list" id="user-mention-list"><li class="header user-mention__header"><h5>in this conversation</h5></li></ul></div></div><div data-role="drag-drop-placeholder" class="media-drag-hover" style="display: none;"><div class="drag-text"> Drag and drop your images here to upload them.</div></div><div class="media-preview empty" data-role="media-preview"><ul data-role="media-progress-list"></ul>
<ul data-role="media-rich-list"></ul>
<div class="media-expanded empty" data-role="media-preview-expanded">
<img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-role="media-preview-expanded-image" alt="Media preview placeholder">
</div>
</div><div class="edit-alert" role="postbox-alert"></div><div class="text-editor-container"><div class="post-actions"><div class="wysiwyg"><div class="gif-picker"><div class="wysiwyg__item" data-role="gif-picker-toggle" title="GIF"><div class="wysiwyg__gif" title="GIF" role="img" aria-label="GIF"></div></div><div class="hidden gif-picker__popout-container" data-role="gif-picker-popout-container"></div><div class="new-feature-badge-star" title="New"><div class="wysiwyg__star-badge wysiwyg__star-badge-dims"></div></div></div><div class="media-uploader"><li class="wysiwyg__item" data-role="media-uploader"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" tabindex="-1" data-action="attach" class="attach" title="Upload Images">
<div class="wysiwyg__attach wysiwyg__attach-dims" role="img" aria-label="A"></div>
</a>
<input type="file" data-role="media-upload" tabindex="-1" accept="image/*">
</li></div><div class="vertical-separator"></div><div data-action="text-editor-buttons"><div class="wysiwyg__item" data-action="text-editor-tag" data-tag="b"><div class="wysiwyg__bold" title="Bold" role="img" aria-label="B"></div></div><div class="wysiwyg__item" data-action="text-editor-tag" data-tag="i"><div class="wysiwyg__italic" title="Italic" role="img" aria-label="I"></div></div><div class="wysiwyg__item" data-action="text-editor-tag" data-tag="u"><div class="wysiwyg__underline" title="Underline" role="img" aria-label="U"></div></div><div class="wysiwyg__item" data-action="text-editor-tag" data-tag="s"><div class="wysiwyg__strikethrough" title="Strikethrough" role="img" aria-label="S"></div></div><div class="wysiwyg__item" data-action="text-editor-tag" data-tag="a"><div class="wysiwyg__link" title="Link" role="img" aria-label="L"></div></div><div class="wysiwyg__item" data-action="text-editor-tag" data-tag="spoiler"><div class="wysiwyg__spoiler" title="Spoiler" role="img" aria-label="Sp"></div></div><div class="wysiwyg__item" data-action="text-editor-tag" data-tag="code"><div class="wysiwyg__code" title="Code" role="img" aria-label="C"></div></div><div class="wysiwyg__item" data-action="text-editor-tag" data-tag="blockquote"><div class="wysiwyg__blockquote" title="Quote" role="img" aria-label="Q"></div></div></div></div><div class="logged-in"><section><div class="temp-post"><button class="btn post-action__button full-size-button hidden">Post as <span data-role="username"></span></button><button class="btn post-action__button small-size-button hidden">Post</button></div></section></div></div></div></div></div></div><div data-role="login-form"><div><div><section class="auth-section logged-out__display"><div class="connect"><h6>Log in with</h6><ul data-role="login-menu" class="services login-buttons"><li class="auth-disqus"><button type="button" data-action="auth:disqus" title="Disqus" class="connect__button"><i class="icon-disqus"></i></button></li><li class="auth-facebook"><button type="button" data-action="auth:facebook" title="Facebook" class="connect__button"><i class="icon-facebook-circle"></i></button></li><li class="auth-twitter"><button type="button" data-action="auth:twitter" title="Twitter" class="connect__button"><i class="icon-twitter-circle"></i></button></li><li class="auth-google"><button type="button" data-action="auth:google" title="Google" class="connect__button"><i class="icon-google-plus-circle"></i></button></li></ul></div><div class="guest"><h6 class="guest-form-title"><span class="register-text"> or sign up with Disqus </span><span class="guest-text"> or pick a name </span></h6> <button type="button" class="help-tooltip__wrapper help-icon" name="guest_tooltip" tabindex="0"><div id="rules" class="help-tooltip__container" data-role="guest-form-tooltip"><div class="tooltip show help-tooltip"><h3 class="help-tooltip__heading">Disqus is a discussion network</h3><ul class="help-tooltip__list"><li><span>Don't be a jerk or do anything illegal. Everything is easier that way.</span></li></ul><p class="clearfix"><a href="https://docs.disqus.com/kb/terms-and-policies/" class="btn btn-small help-tooltip__button" rel="noopener noreferrer" target="_blank">Read full terms and conditions</a></p></div></div></button><p class="input-wrapper"><input dir="auto" type="text" placeholder="Name" name="display_name" id="view63_display_name" maxlength="30" class="input--text" aria-label="name"></p><div class="guest-details " data-role="guest-details"><p class="input-wrapper"><input dir="auto" type="email" placeholder="Email" name="email" id="view63_email" class="input--text" aria-label="email"></p><p class="input-wrapper"><input dir="auto" disabled="" type="text" class="register-text input--text" placeholder="Password" name="password" aria-label="password" id="view63_password"></p><div class="acceptance-wrapper"><p><label><input type="checkbox" name="tos"><span class="spacing-left-small">I agree to Disqus' <a href="https://help.disqus.com/customer/portal/articles/466260-terms-of-service" target="_blank" rel="noopener noreferrer">Terms of Service</a></span></label></p><p><label><input type="checkbox" name="privacy-policy"><span class="spacing-left-small">I agree to Disqus' processing of email and IP address, and the use of cookies, to facilitate my authentication and posting of comments, explained further in the <a href="https://help.disqus.com/customer/portal/articles/466259-privacy-policy" target="_blank" rel="noopener noreferrer">Privacy Policy</a></span></label></p><p><label><input type="checkbox" name="data-sharing"><span class="spacing-left-small">I agree to additional processing of my information, including first and third party cookies, for personalized content and advertising as outlined in our <a href="https://disqus.com/data-sharing-settings/">Data Sharing Policy</a></span></label></p></div><div class="guest-checkbox"><label><input type="checkbox" name="author-guest"> I'd rather post as a guest</label></div><div class="g-recaptcha" data-role="grecaptcha-container"></div><div class="proceed" data-role="submit-btn-container"><div><button type="submit" class="proceed__button btn submit" aria-label="Post" disabled="disabled"><span class="icon-proceed"></span><div class="spinner"></div></button><button type="submit" class="proceed__button btn next" aria-label="Next" disabled="disabled"><span class="icon-proceed"></span><div class="spinner"></div></button></div></div></div></div></section></div></div></div></div></form></div><button class="alert alert--realtime" data-role="realtime-notification" style="display: none;"></button><div id="email-signup"></div><div id="no-posts" style="display: none;"></div><ul id="post-list" class="post-list"><li class="post" id="post-3747086801"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3747086801"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span></span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/navneetmkumar/" data-action="profile" data-tab="" data-username="navneetmkumar" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="279442539" src="./noavatar92.png" data-src="//a.disquscdn.com/1638827995/images/noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/navneetmkumar/" data-action="profile" data-tab="" data-username="navneetmkumar" target="_blank" rel="noopener noreferrer">Navneet M Kumar</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true"></span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-3747086801" data-role="relative-time" class="time-ago" title="Wednesday, February 7, 2018 4:48 PM">4 years ago</a> <span><span class="bullet time-ago-bullet" aria-hidden="true"></span> <span class="has-edit" data-role="has-edit">edited</span></span></span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>t=0T1[(log(at|st))2][(Rt()b(st))2]</p><p>Shouldnt the optimal baseline be obtained by differentiating this quantity( the variance) and finding the minima which would not be similar to the expectation of the rewards or the average trajectory ?</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a><div class="post-votes__separator"></div><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="bullet" aria-hidden="true"></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true"></li><li id="comment__share-3747086801" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="expand-share"><span class="text">Share </span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter" data-action="share:twitter"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1pyx2oh" name="Link" title="Click to copy post link" data-action="copy-link"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3747086801"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4293134529"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="4293134529"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span></span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/seitad/" data-action="profile" data-tab="" data-username="seitad" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="157207291" src="./noavatar92.png" data-src="https://c.disquscdn.com/uploads/users/15720/7291/avatar92.jpg?1452441681" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/seitad/" data-action="profile" data-tab="" data-username="seitad" target="_blank" rel="noopener noreferrer">Daniel Seita</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-3747086801" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Navneet M Kumar</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true"></span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4293134529" data-role="relative-time" class="time-ago" title="Saturday, January 19, 2019 5:42 AM">3 years ago</a> <span><span class="bullet time-ago-bullet" aria-hidden="true"></span> <span class="has-edit" data-role="has-edit">edited</span></span></span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>I don't think it's as simple as that. There is a very hefty theory that Bartlett and others did many years ago that derived the optimal minimum variance. I have not had time to go through those papers because it's not directly relevant to my work.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a><div class="post-votes__separator"></div><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="bullet" aria-hidden="true"></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true"></li><li id="comment__share-4293134529" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="expand-share"><span class="text">Share </span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter" data-action="share:twitter"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1z00rsx" name="Link" title="Click to copy post link" data-action="copy-link"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4293134529"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4293134529-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3747086801-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4001599207"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="4001599207"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span></span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/damienshen/" data-action="profile" data-tab="" data-username="damienshen" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="215271782" src="./noavatar92.png" data-src="//a.disquscdn.com/1638827995/images/noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/damienshen/" data-action="profile" data-tab="" data-username="damienshen" target="_blank" rel="noopener noreferrer">Damien Shen</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true"></span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4001599207" data-role="relative-time" class="time-ago" title="Monday, July 23, 2018 2:29 AM">3 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks a lot. It really help me to understand the concept better</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a><div class="post-votes__separator"></div><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="bullet" aria-hidden="true"></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true"></li><li id="comment__share-4001599207" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="expand-share"><span class="text">Share </span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter" data-action="share:twitter"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1u6g5pj" name="Link" title="Click to copy post link" data-action="copy-link"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4001599207"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4293132104"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="4293132104"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span></span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/seitad/" data-action="profile" data-tab="" data-username="seitad" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="157207291" src="./noavatar92.png" data-src="https://c.disquscdn.com/uploads/users/15720/7291/avatar92.jpg?1452441681" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/seitad/" data-action="profile" data-tab="" data-username="seitad" target="_blank" rel="noopener noreferrer">Daniel Seita</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4001599207" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Damien Shen</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true"></span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4293132104" data-role="relative-time" class="time-ago" title="Saturday, January 19, 2019 5:38 AM">3 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>You're welcome!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a><div class="post-votes__separator"></div><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="bullet" aria-hidden="true"></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true"></li><li id="comment__share-4293132104" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="expand-share"><span class="text">Share </span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter" data-action="share:twitter"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1z00pxk" name="Link" title="Click to copy post link" data-action="copy-link"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4293132104"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4293132104-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4001599207-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4129062643"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="4129062643"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span></span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/ge_yang/" data-action="profile" data-tab="" data-username="ge_yang" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="104007738" src="./noavatar92.png" data-src="https://c.disquscdn.com/uploads/users/10400/7738/avatar92.jpg?1538675635" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/ge_yang/" data-action="profile" data-tab="" data-username="ge_yang" target="_blank" rel="noopener noreferrer">Ge Yang</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true"></span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4129062643" data-role="relative-time" class="time-ago" title="Thursday, October 4, 2018 7:53 PM">3 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>The three ways are not equivalent algebraically, and I think this is an important distinction. They are not equivalent, because ii and iii makes assumptions of the graphical model.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a><div class="post-votes__separator"></div><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="bullet" aria-hidden="true"></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true"></li><li id="comment__share-4129062643" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="expand-share"><span class="text">Share </span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter" data-action="share:twitter"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1wac54j" name="Link" title="Click to copy post link" data-action="copy-link"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4129062643"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4293132836"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="4293132836"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span></span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/seitad/" data-action="profile" data-tab="" data-username="seitad" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="157207291" src="./noavatar92.png" data-src="https://c.disquscdn.com/uploads/users/15720/7291/avatar92.jpg?1452441681" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/seitad/" data-action="profile" data-tab="" data-username="seitad" target="_blank" rel="noopener noreferrer">Daniel Seita</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4129062643" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Ge Yang</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true"></span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4293132836" data-role="relative-time" class="time-ago" title="Saturday, January 19, 2019 5:39 AM">3 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Sorry for my late response. What are you referring to in the post when you mention "three ways"?</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a><div class="post-votes__separator"></div><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="bullet" aria-hidden="true"></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true"></li><li id="comment__share-4293132836" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="expand-share"><span class="text">Share </span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter" data-action="share:twitter"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1z00qhw" name="Link" title="Click to copy post link" data-action="copy-link"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4293132836"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4293132836-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4129062643-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4142228096"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="4142228096"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span></span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_Jk9goUcHDo/" data-action="profile" data-tab="" data-username="disqus_Jk9goUcHDo" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="149082056" src="./noavatar92.png" data-src="https://c.disquscdn.com/uploads/users/14908/2056/avatar92.jpg?1539378518" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_Jk9goUcHDo/" data-action="profile" data-tab="" data-username="disqus_Jk9goUcHDo" target="_blank" rel="noopener noreferrer">krishnan</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true"></span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4142228096" data-role="relative-time" class="time-ago" title="Friday, October 12, 2018 11:08 PM">3 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks so much for putting this together! It was very helpful when trying to learn about these topics.  One thing I had a bit of confusion about: when you introduce the baseline into the calculation of expectation of cumulative reward of the trajectory, the left-hand side is technically no longer the expectation of R(tau), correct? It is now the expectation of a new estimate, R'(tau), which has less variance.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a><div class="post-votes__separator"></div><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="bullet" aria-hidden="true"></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true"></li><li id="comment__share-4142228096" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="expand-share"><span class="text">Share </span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter" data-action="share:twitter"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1wi6bnk" name="Link" title="Click to copy post link" data-action="copy-link"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4142228096"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4293133321"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="4293133321"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span></span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/seitad/" data-action="profile" data-tab="" data-username="seitad" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="157207291" src="./noavatar92.png" data-src="https://c.disquscdn.com/uploads/users/15720/7291/avatar92.jpg?1452441681" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/seitad/" data-action="profile" data-tab="" data-username="seitad" target="_blank" rel="noopener noreferrer">Daniel Seita</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4142228096" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> krishnan</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true"></span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4293133321" data-role="relative-time" class="time-ago" title="Saturday, January 19, 2019 5:40 AM">3 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>The baseline itself should maintain equality if the original expression was an equality. The *discount* factors make it an inequality/approximation. Which specific area of the post are you referring to?</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a><div class="post-votes__separator"></div><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="bullet" aria-hidden="true"></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true"></li><li id="comment__share-4293133321" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="expand-share"><span class="text">Share </span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter" data-action="share:twitter"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1z00qvd" name="Link" title="Click to copy post link" data-action="copy-link"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4293133321"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4293133321-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4142228096-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4267479510"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="4267479510"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span></span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar"><div class="user"><img src="./noavatar92.png" class="user" alt="Avatar"></div></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span class="author">Sudharshan</span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true"></span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4267479510" data-role="relative-time" class="time-ago" title="Thursday, January 3, 2019 2:12 PM">3 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Really good effort.<br>I have a small doubt,how is the "Reward to go" actually calculated in Deep Reinforcement Learning.Let it be the Monte Carlo reward or Q values or the advantage.How can we estimate the reward for future state from the current state without knowing the future states.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a><div class="post-votes__separator"></div><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="bullet" aria-hidden="true"></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true"></li><li id="comment__share-4267479510" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="expand-share"><span class="text">Share </span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter" data-action="share:twitter"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1ykqw9i" name="Link" title="Click to copy post link" data-action="copy-link"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4267479510"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4293133852"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="4293133852"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span></span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/seitad/" data-action="profile" data-tab="" data-username="seitad" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="157207291" src="./noavatar92.png" data-src="https://c.disquscdn.com/uploads/users/15720/7291/avatar92.jpg?1452441681" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/seitad/" data-action="profile" data-tab="" data-username="seitad" target="_blank" rel="noopener noreferrer">Daniel Seita</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4267479510" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Sudharshan</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true"></span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4293133852" data-role="relative-time" class="time-ago" title="Saturday, January 19, 2019 5:40 AM">3 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>You play the future states and compute the rewards after the trajectory is done. That's how it's often implemented in practice. Is that clear?</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a><div class="post-votes__separator"></div><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="bullet" aria-hidden="true"></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true"></li><li id="comment__share-4293133852" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="expand-share"><span class="text">Share </span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter" data-action="share:twitter"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1z00ra4" name="Link" title="Click to copy post link" data-action="copy-link"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4293133852"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4293133852-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4267479510-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4293481946"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="4293481946"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span></span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_dp7d6xECXP/" data-action="profile" data-tab="" data-username="disqus_dp7d6xECXP" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="327750135" src="./noavatar92.png" data-src="//a.disquscdn.com/1638827995/images/noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_dp7d6xECXP/" data-action="profile" data-tab="" data-username="disqus_dp7d6xECXP" target="_blank" rel="noopener noreferrer">mike</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true"></span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4293481946" data-role="relative-time" class="time-ago" title="Saturday, January 19, 2019 2:36 PM">3 years ago</a> <span><span class="bullet time-ago-bullet" aria-hidden="true"></span> <span class="has-edit" data-role="has-edit">edited</span></span></span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>In approximation (i) of variance reduction proof part, E[X] seems to be 0 so it is ignored. Is this because the exception of log function's derivative is 0, so no matter it multiply b(s_t) or [R(\tau)-b(s_t)] is still 0?</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a><div class="post-votes__separator"></div><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="bullet" aria-hidden="true"></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true"></li><li id="comment__share-4293481946" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="expand-share"><span class="text">Share </span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter" data-action="share:twitter"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1z087ve" name="Link" title="Click to copy post link" data-action="copy-link"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4293481946"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4922842103"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="4922842103"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span></span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/pastafarianist/" data-action="profile" data-tab="" data-username="pastafarianist" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="112705586" src="./noavatar92.png" data-src="https://c.disquscdn.com/uploads/users/11270/5586/avatar92.jpg?1590102352" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/pastafarianist/" data-action="profile" data-tab="" data-username="pastafarianist" target="_blank" rel="noopener noreferrer">Pastafarianist</a></span> </span><span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4293481946" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> mike</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true"></span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4922842103" data-role="relative-time" class="time-ago" title="Friday, May 22, 2020 1:05 AM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>This seems to be a mistake, since, as far as I understand, E[X] is precisely the policy gradient.</p><p>In addition, there is an (insignificant) typo in the section "How to Introduce a Baseline" under "Step (ii)". Twice, a formula says \sum_{t'}, whereas it should say \sum_{t'=0}.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a><div class="post-votes__separator"></div><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="bullet" aria-hidden="true"></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true"></li><li id="comment__share-4922842103" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="expand-share"><span class="text">Share </span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter" data-action="share:twitter"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/29exl9z" name="Link" title="Click to copy post link" data-action="copy-link"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4922842103"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4922842103-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4293481946-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4299681379"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="4299681379"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span></span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar"><div class="user"><img src="./noavatar92.png" class="user" alt="Avatar"></div></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span class="author">Sebastian Lee</span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true"></span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4299681379" data-role="relative-time" class="time-ago" title="Monday, January 21, 2019 11:40 AM">3 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>The exchanging of the integral and derivative in the log-derivative trick is permissible in this way because the integral - being over a probability space - is between negative infinity and positive infinity i.e. the integral bounds are constants and not functions of \theta. This means that when differentiating under the integral (Leibnitz trick) the two terms involving the partial derivatives of the integral bounds disappear and you are left with the term where the integral and differential have simply swapped places.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-up  count-3" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">3</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a><div class="post-votes__separator"></div><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="bullet" aria-hidden="true"></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true"></li><li id="comment__share-4299681379" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="expand-share"><span class="text">Share </span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter" data-action="share:twitter"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1z3x3dv" name="Link" title="Click to copy post link" data-action="copy-link"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4299681379"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4724105575"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="4724105575"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span></span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_k5RkXja2sG/" data-action="profile" data-tab="" data-username="disqus_k5RkXja2sG" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="341978210" src="./noavatar92.png" data-src="//a.disquscdn.com/1638827995/images/noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_k5RkXja2sG/" data-action="profile" data-tab="" data-username="disqus_k5RkXja2sG" target="_blank" rel="noopener noreferrer">Anonymous</a></span> </span><span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4299681379" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Sebastian Lee</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true"></span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4724105575" data-role="relative-time" class="time-ago" title="Saturday, December 14, 2019 3:08 PM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Reference: <a href="https://disq.us/url?url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FLeibniz_integral_rule%3A6FkLwDswhM2fG48GVqoUqPJ6v-A&amp;cuid=3598555" rel="nofollow noopener" title="https://en.wikipedia.org/wiki/Leibniz_integral_rule">https://en.wikipedia.org/wi...</a></p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">1</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a><div class="post-votes__separator"></div><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="bullet" aria-hidden="true"></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true"></li><li id="comment__share-4724105575" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="expand-share"><span class="text">Share </span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter" data-action="share:twitter"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/264lz6v" name="Link" title="Click to copy post link" data-action="copy-link"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4724105575"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4754906193"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="4754906193"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span></span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/seitad/" data-action="profile" data-tab="" data-username="seitad" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="157207291" src="./noavatar92.png" data-src="https://c.disquscdn.com/uploads/users/15720/7291/avatar92.jpg?1452441681" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/seitad/" data-action="profile" data-tab="" data-username="seitad" target="_blank" rel="noopener noreferrer">Daniel Seita</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4724105575" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Anonymous</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true"></span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4754906193" data-role="relative-time" class="time-ago" title="Monday, January 13, 2020 12:47 AM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a><div class="post-votes__separator"></div><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="bullet" aria-hidden="true"></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true"></li><li id="comment__share-4754906193" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="expand-share"><span class="text">Share </span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter" data-action="share:twitter"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/26my53l" name="Link" title="Click to copy post link" data-action="copy-link"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4754906193"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4754906193-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4724105575-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4299681379-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4352851225"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="4352851225"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span></span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar"><div class="user"><img src="./noavatar92.png" class="user" alt="Avatar"></div></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span class="author">Cuong Hoang</span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true"></span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4352851225" data-role="relative-time" class="time-ago" title="Monday, February 25, 2019 4:12 AM">3 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Hi,<br>Thanks for your great blog. I was wondering one thing: what are the benefits of using advantage function instead of R_t() and b(s_t)? Are they the same? This somehow makes me confusing. Thanks!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a><div class="post-votes__separator"></div><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="bullet" aria-hidden="true"></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true"></li><li id="comment__share-4352851225" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="expand-share"><span class="text">Share </span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter" data-action="share:twitter"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1zzkpi1" name="Link" title="Click to copy post link" data-action="copy-link"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4352851225"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4352851225-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4892871874"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="4892871874"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span></span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/vigneshskannan/" data-action="profile" data-tab="" data-username="vigneshskannan" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="139371709" src="./noavatar92.png" data-src="https://c.disquscdn.com/uploads/users/13937/1709/avatar92.jpg?1588064661" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/vigneshskannan/" data-action="profile" data-tab="" data-username="vigneshskannan" target="_blank" rel="noopener noreferrer">Vignesh S Kannan</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true"></span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4892871874" data-role="relative-time" class="time-ago" title="Tuesday, April 28, 2020 11:04 AM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thank you so much. I was stuck trying to prove the baseline is unbiased when it's a function of the state. I spent a whole day on it. And only this blog finally unblocked me. Cheers !!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a><div class="post-votes__separator"></div><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="bullet" aria-hidden="true"></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true"></li><li id="comment__share-4892871874" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="expand-share"><span class="text">Share </span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter" data-action="share:twitter"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/28x383m" name="Link" title="Click to copy post link" data-action="copy-link"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4892871874"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4923679941"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="4923679941"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span></span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/seitad/" data-action="profile" data-tab="" data-username="seitad" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="157207291" src="./noavatar92.png" data-src="https://c.disquscdn.com/uploads/users/15720/7291/avatar92.jpg?1452441681" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/seitad/" data-action="profile" data-tab="" data-username="seitad" target="_blank" rel="noopener noreferrer">Daniel Seita</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4892871874" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Vignesh S Kannan</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true"></span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4923679941" data-role="relative-time" class="time-ago" title="Friday, May 22, 2020 6:23 PM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a><div class="post-votes__separator"></div><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="bullet" aria-hidden="true"></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true"></li><li id="comment__share-4923679941" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="expand-share"><span class="text">Share </span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter" data-action="share:twitter"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/29ffjr9" name="Link" title="Click to copy post link" data-action="copy-link"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4923679941"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4923679941-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4892871874-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4993879335"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="4993879335"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span></span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/nathanraynalcastang/" data-action="profile" data-tab="" data-username="nathanraynalcastang" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="353124300" src="./noavatar92.png" data-src="//a.disquscdn.com/1638827995/images/noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/nathanraynalcastang/" data-action="profile" data-tab="" data-username="nathanraynalcastang" target="_blank" rel="noopener noreferrer">Nathan Raynal-Castang</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true"></span> <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/#comment-4993879335" data-role="relative-time" class="time-ago" title="Thursday, July 16, 2020 11:39 AM">a year ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks a lot Daniel ! One of the clearest explanation I encountered on the subject. I have a small remark to make though. In the part "How to introduce a Baseline" , you introduce a way to put the derivative inside the sum (equations (i), (ii), and (iii)). But after equation (i), can't you just simply use the linearity property of the derivative to switch the sum and the derivative ? It changes the sum over the rewards in equation (iii), from t' to T-1, to 0 to T-1. It does not influence the rest of your article though, so it's no big deal, but I just thought this part might be a bit over-complicated compared to simply using linearity property. Maybe I'm missing something ?</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a><div class="post-votes__separator"></div><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="bullet" aria-hidden="true"></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true"></li><li id="comment__share-4993879335" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="expand-share"><span class="text">Share </span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter" data-action="share:twitter"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/2al85yf" name="Link" title="Click to copy post link" data-action="copy-link"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4993879335"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4993879335-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="load-more" data-role="more" style="display: none;"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" data-action="more-posts" class="btn load-more__button">Load more comments</a></div></div></section><div id="placement-bottom" data-tracking-area="discovery-south"></div><footer id="footer" data-tracking-area="footer" class="disqus-footer__wrapper"><div class="disqus-footer"><ul class="disqus-footer__list"><li id="thread-subscribe-button" class="email disqus-footer__item"><div class="default"><a href="https://disqus.com/embed/comments/?base=default&amp;f=seitasplace&amp;t_u=https%3A%2F%2Fdanieltakeshi.github.io%2F2017%2F03%2F28%2Fgoing-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients%2F&amp;t_d=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;t_t=Going%20Deeper%20Into%20Reinforcement%20Learning%3A%20Fundamentals%20of%20Policy%20Gradients&amp;s_o=default#" rel="nofollow" data-action="subscribe" title="Subscribe and get email updates from this discussion" class="disqus-footer__link"><i aria-hidden="true" class="icon icon-mail"></i><span class="clip">Subscribe</span><i aria-hidden="true" class="icon icon-checkmark"></i></a></div></li><li class="install disqus-footer__item"><a href="https://publishers.disqus.com/engage?utm_source=seitasplace&amp;utm_medium=Disqus-Footer" rel="nofollow noopener noreferrer" target="_blank" class="disqus-footer__link"><i aria-hidden="true" class="icon icon-disqus"></i><span class="clip hidden-md">Add Disqus to your site</span><span class="clip visible-md hidden-xs">Add Disqus</span><span class="clip visible-xs">Add</span></a></li><li class="do-not-sell disqus-footer__item"><a href="https://disqus.com/data-sharing-settings/" rel="nofollow noopener noreferrer" target="_blank" class="disqus-footer__link"><i aria-hidden="true" class="icon icon-warning"></i><span class="clip">Do Not Sell My Data</span></a></li></ul><span class="disqus-footer__logo"><a href="https://disqus.com/" rel="nofollow" title="Powered by Disqus" class="disqus-footer__link">Powered by Disqus</a></span></div></footer></div></div></div></div></body></html>